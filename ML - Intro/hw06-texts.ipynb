{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvOD02o3HvH"
      },
      "source": [
        "# Домашнее задание 6: классификация текстов"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwj_Iie3HvJ"
      },
      "source": [
        "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
        "\n",
        "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data \n",
        "\n",
        "\n",
        "Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQVgqLg93HvJ"
      },
      "source": [
        "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TcjEYh7R3HvK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import  List\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mjwffGiB3HvK"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v0uUoFTN3HvK",
        "outputId": "0d37d677-a00d-449c-8f86-6f85dce2ef4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f49NdWY23HvL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.3, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLemInT3HvL"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "\n",
        "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "96aJxmkV4105"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.isnull().any() # есть пропуски keyword и location\n",
        "test.isnull().any()  # аналогично\n",
        "\n",
        "# заполнили пропуски \n",
        "train.fillna('', inplace=True)\n",
        "test.fillna('', inplace=True)\n",
        "\n",
        "# теперь все хорошо\n",
        "train.isnull().any().any() # False \n",
        "test.isnull().any().any()  # False "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A8CPBUal3HvL"
      },
      "source": [
        "## Задание 2 (1 балл)\n",
        "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
        "\n",
        "1. Какое распределение классов в обучающей выборке?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WvJ_EU9o5BGm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Класс 0: 56.75%\n",
            "Класс 1: 43.25%\n"
          ]
        }
      ],
      "source": [
        "# train.target.unique() - убедились, что класса всего два [0, 1]\n",
        "total_count = train.shape[0]\n",
        "false_count = (train.target == 0).sum()\n",
        "print(f\"Класс 0: {round(100 * false_count/total_count, 2)}%\")\n",
        "print(f\"Класс 1: {100 - round(100 * false_count/total_count, 2)}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f08KScbP5q2y"
      },
      "source": [
        "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WSCb0htu5w_Y"
      },
      "outputs": [],
      "source": [
        "popular_keywords = np.array(train.keyword.value_counts()[1:11].index) # самое популярное - пустое значение, его пропустили"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "r9wSwm4L9REm",
        "outputId": "3c6827d4-d35f-47ac-c756-6ab4740ed0b9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplots(figsize=(14,8))\n",
        "sns.histplot(data=train[train.keyword.isin(popular_keywords)], x=\"keyword\", hue=\"target\", multiple=\"stack\")\n",
        "plt.xlabel(\"Keywords\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend([\"Negative\", \"Positive\"])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4al3g9E-P09"
      },
      "source": [
        "**Выводы**: \n",
        "Целевая переменная очень хорошо перемешана между различными ключевыми словами, так что по ним не удастся провести классификацию. Исключениями являются лишь некоторые слова, но они покрывают очень маленькую часть датасета, чтобы предсказывать что-то."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c006nNBP3HvM"
      },
      "source": [
        "## Задание 3 (0.5 балла) \n",
        "\n",
        "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "GdF9gFmL-c0r",
        "outputId": "ebe008b7-4d0e-4bc2-89f3-60a575c1b1fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6590</th>\n",
              "      <td>9436</td>\n",
              "      <td>survivors</td>\n",
              "      <td>Marietta, GA</td>\n",
              "      <td>Stemming from my #Cubs talk- the team rosters ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7122</th>\n",
              "      <td>10203</td>\n",
              "      <td>violent%20storm</td>\n",
              "      <td></td>\n",
              "      <td>If you were the NWS wth a rotating storm w/ a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2769</th>\n",
              "      <td>3980</td>\n",
              "      <td>devastation</td>\n",
              "      <td>Atlanta g.a.</td>\n",
              "      <td>http://t.co/Gxgm1T3W0J From Devastation to Ela...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id          keyword      location  \\\n",
              "6590   9436        survivors  Marietta, GA   \n",
              "7122  10203  violent%20storm                 \n",
              "2769   3980      devastation  Atlanta g.a.   \n",
              "\n",
              "                                                   text  target  \n",
              "6590  Stemming from my #Cubs talk- the team rosters ...       1  \n",
              "7122  If you were the NWS wth a rotating storm w/ a ...       1  \n",
              "2769  http://t.co/Gxgm1T3W0J From Devastation to Ela...       0  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.loc[[6590, 7122, 2769]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lwwJKX_l-eoh"
      },
      "outputs": [],
      "source": [
        "train_new = train.copy()\n",
        "train_new['text'] = train[['keyword', 'location', 'text']].agg(' '.join, axis=1)\n",
        "\n",
        "train_new.drop(columns=[\"id\", \"keyword\", \"location\"], inplace=True)\n",
        "\n",
        "test_new = test.copy()\n",
        "test_new['text'] = test[['keyword', 'location', 'text']].agg(' '.join, axis=1)\n",
        "\n",
        "test_new.drop(columns=[\"id\", \"keyword\", \"location\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "jk7P70XX_CpT",
        "outputId": "5cf01b29-8ada-46d7-f7ee-74e7aed37996"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6590</th>\n",
              "      <td>survivors Marietta, GA Stemming from my #Cubs ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7122</th>\n",
              "      <td>violent%20storm  If you were the NWS wth a rot...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2769</th>\n",
              "      <td>devastation Atlanta g.a. http://t.co/Gxgm1T3W0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  target\n",
              "6590  survivors Marietta, GA Stemming from my #Cubs ...       1\n",
              "7122  violent%20storm  If you were the NWS wth a rot...       1\n",
              "2769  devastation Atlanta g.a. http://t.co/Gxgm1T3W0...       0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_new.loc[[6590, 7122, 2769]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ViXdGTxP3HvM"
      },
      "source": [
        "## Задание 4 (0.5 балла)\n",
        "\n",
        "Далее мы будем пока работать только с train частью.\n",
        "\n",
        "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
        "2. Какого размера получилась матрица?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oB1MTqUVAbPA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 18455)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cnt_vec = CountVectorizer()\n",
        "matrix = cnt_vec.fit_transform(train_new.text)\n",
        "matrix.shape # очень много слов"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A4waLlnC3HvM"
      },
      "source": [
        "## Задание 5 (1 балл)\n",
        "\n",
        "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
        "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
        "\n",
        "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
        "\n",
        "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
        "\n",
        "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
        "\n",
        "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7PhQSWqcHhU8"
      },
      "outputs": [],
      "source": [
        "def contains_digit(s: str) -> bool:\n",
        "    return any(l.isdigit() for l in s)\n",
        "\n",
        "def contains_punctuation(s: str) -> bool:\n",
        "    punc = '''!()-[]{};:'\"\\\\,<>./?$%^&@#*_~`'''\n",
        "    for l in s:\n",
        "        if l in punc:\n",
        "            return True \n",
        "    return False \n",
        "\n",
        "def is_hashtag(s: str) -> bool:\n",
        "    return s[0] == \"#\"\n",
        "\n",
        "def is_mention(s: str) -> bool:\n",
        "    return s[0] == \"@\" and s[1:].isalnum()\n",
        "\n",
        "def investigate_vocabulary(vocabulary):\n",
        "    counts = {l: 0 for l in \"dphm\" }\n",
        "    for word in vocabulary:\n",
        "        counts[\"d\"] += contains_digit(word)\n",
        "        counts[\"p\"] += contains_punctuation(word)\n",
        "        counts[\"h\"] += is_hashtag(word)\n",
        "        counts[\"m\"] += is_mention(word)\n",
        "    \n",
        "    print('With digit:      ', counts[\"d\"]\n",
        "          )\n",
        "    print('With punctuation:', counts[\"p\"]\n",
        "          )\n",
        "    print('Hashtags:        ', counts[\"h\"]\n",
        "          )\n",
        "    print('Mentions:        ', counts[\"m\"]\n",
        "          )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8hL5ZcQIdp6",
        "outputId": "c8b50455-38f2-455d-c9f6-96b03a7e09ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With digit:       2\n",
            "With punctuation: 7\n",
            "Hashtags:         1\n",
            "Mentions:         3\n"
          ]
        }
      ],
      "source": [
        "dummy_vocab = {'th1nk' : 0,\n",
        "               'think333' : 1,\n",
        "               'think.' : 2,\n",
        "               'th!nk' : 3,\n",
        "               'th...nk' : 4,\n",
        "               '#think' : 5,\n",
        "               '@think' : 6,\n",
        "               '@thinking':7,\n",
        "               '@nothink' : 8,\n",
        "               'think' : 9}\n",
        "investigate_vocabulary(dummy_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VpW8R_SuKR_l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With digit:       3812\n",
            "With punctuation: 315\n",
            "Hashtags:         0\n",
            "Mentions:         0\n"
          ]
        }
      ],
      "source": [
        "investigate_vocabulary( cnt_vec.vocabulary_\n",
        "                       )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bfNLaxX93HvM"
      },
      "source": [
        "## Задание 6 (0.5 балла)\n",
        "\n",
        "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xnlRoXUS3HvM"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "# Чтобы узнать, какие параметры есть у этого токенайзера - используйте help(TweetTokenizer)\n",
        "# Для того, чтобы передать токенайзер в CountVectorizer используйте параметр tokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "texts = texts_tokenized = [\n",
        "    \" \".join([w for w in tknzr.tokenize(t) if w.isalpha()]) for t in train_new.text\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8lPMIf6UKccT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 11702)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnt_vec = CountVectorizer() \n",
        "matrix = cnt_vec.fit_transform(texts)\n",
        "matrix.shape # на треть меньше"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7oR5kNKVLLpm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With digit:       0\n",
            "With punctuation: 0\n",
            "Hashtags:         0\n",
            "Mentions:         0\n"
          ]
        }
      ],
      "source": [
        "investigate_vocabulary(cnt_vec.vocabulary_\n",
        "                       )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wetr80-ILULV"
      },
      "source": [
        "**Сравнение:** \n",
        "Мы уменьшили размеры массива векторов слов и избавились от лишних символов, которые не несут смысловой нагрузки для нашей задачи. Размер массива уменьшился приблизительно на треть, но есть слова, которые повторяются в разных формах."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_k_-i1x3HvM"
      },
      "source": [
        "## Задание 7 (2 балла)\n",
        "\n",
        "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
        "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
        "\n",
        "0. Приведет все буквы к нижнему регистру\n",
        "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
        "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
        "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
        "4. Проведет стемминг с помощью SnowballStemmer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qhwmi7DEMD25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\TechnoDX\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk.corpus as corpus\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tknzr = TweetTokenizer()\n",
        "stopwords = corpus.stopwords.words('english')\n",
        "punc = '''!()-[]{};:'\"\\\\,<>|./?$%^&@#*_~`'''\n",
        "\n",
        "def contains_only_latin_letters(s: str) -> bool:\n",
        "    return bool(re.match(r'^[a-zA-Z]+$', s))\n",
        "\n",
        "def is_emoji(s: str) -> bool:\n",
        "    b = \"(){}[]\"\n",
        "    return all(l in punc for l in s) and any(l in b for l in s)\n",
        "\n",
        "def is_hashtag(s: str) -> bool:\n",
        "    return s[0] == '#' and s[1:].isalpha()\n",
        "\n",
        "def custom_tokenizer(s: str) -> List[str]:  \n",
        "    arr = tknzr.tokenize(s)  \n",
        "    res = []\n",
        "    for word in arr:\n",
        "        if is_emoji(word):\n",
        "            res.append(word)\n",
        "            continue\n",
        "        if is_hashtag(word):\n",
        "            res.append(word.lower())\n",
        "            continue\n",
        "        word = stemmer.stem(word.lower())\n",
        "        if not contains_only_latin_letters(word):\n",
        "            continue\n",
        "        if not word in stopwords:\n",
        "            res.append(word)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYzPZzf8O6vj",
        "outputId": "b1107f8a-eef7-49f7-8ed3-910be791d645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['love', 'paint', ':-)', '#art']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_tokenizer('She LOVES painting :-) #art')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2S_-ThAf5It4"
      },
      "source": [
        "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A1fh3_itPz7D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bridg', 'ash', 'australia', 'collaps', 'trent', 'bridg', 'among', 'worst', 'histori', 'england', 'bundl', 'australia']\n",
            "['hail', 'carol', 'stream', 'illinoi', 'great', 'michigan', 'techniqu', 'camp', 'thank', '#goblue', '#wrestleon']\n",
            "['polic', 'houston', 'cnn', 'tennesse', 'movi', 'theater', 'shoot', 'suspect', 'kill', 'polic']\n",
            "['riot', 'still', 'riot', 'coupl', 'hour', 'left', 'class']\n",
            "['wound', 'lake', 'highland', 'crack', 'path', 'wipe', 'morn', 'dure', 'beach', 'run', 'surfac', 'wound', 'left', 'elbow', 'right', 'knee']\n",
            "['airplan', 'somewher', 'expert', 'franc', 'begin', 'examin', 'airplan', 'debri', 'found', 'reunion', 'island', 'french', 'air', 'accid', 'expert', '#mlb']\n",
            "['bloodi', 'isol', 'citi', 'world', 'perth', 'came', 'kill', 'indian', 'fun', 'video', 'smirk', 'remorseless', 'pakistani', 'killer', 'show', 'boast']\n",
            "['burn', 'except', 'idk', 'realli', 'burn']\n",
            "['destroy', '(', 'ask', ')', 'destroy', 'hous']\n",
            "['wound', 'maracay', 'nirgua', 'venezuela', 'polic', 'offic', 'wound', 'suspect', 'dead', 'exchang', 'shot']\n"
          ]
        }
      ],
      "source": [
        "print(*map(custom_tokenizer, train_new[:10].text), sep=\"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a5lNZ4tb3HvN"
      },
      "source": [
        "## Задание 8 (1 балл)\n",
        "\n",
        "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LDqixz7QQEbn"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, token_pattern=None\n",
        "                             )\n",
        "def process(train, test, vectorizer):\n",
        "    train_X = vectorizer.fit_transform(train.text) \n",
        "    test_X = vectorizer.transform(test.text) \n",
        "    train_y = train_new.target\n",
        "    test_y = test_new.target\n",
        "\n",
        "    scaler = MaxAbsScaler()\n",
        "    train_X = scaler.fit_transform(train_X)\n",
        "    test_X = scaler.transform(test_X)\n",
        "    return train_X, train_y, test_X, test_y \n",
        "\n",
        "train_X, train_y, test_X, test_y = process(train_new, test_new, vectorizer)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YcetwuEi5ds9"
      },
      "source": [
        "2. Обучите LogisticRegression на полученных признаках.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BVj03QV2QbWl"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "regression = LogisticRegression(max_iter=1000, random_state=42)\n",
        "regression.fit(train_X, train_y);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ch6uz2P5e-T"
      },
      "source": [
        "3. Посчитайте метрику f1-score на тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osyC0pdT3cSD",
        "outputId": "cd957d5f-5118-4b7f-d7ba-01bb5a524086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7472\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regression.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aFIEYOMZ3HvN"
      },
      "source": [
        "## Задание 9 (1 балл)\n",
        "\n",
        "1. Повторите 8 задание, но с tf-idf векторизатором. Как изменилось качество?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDqs61hl3ve3",
        "outputId": "fdd72125-dfde-4183-d614-3b0bc9002ada"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TechnoDX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7446\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=custom_tokenizer,\n",
        ")\n",
        "\n",
        "train_X, train_y, test_X, test_y = process(train_new, test_new, vectorizer)\n",
        "\n",
        "regressionTI = LogisticRegression(max_iter=1000, random_state=42)\n",
        "regressionTI.fit(train_X, train_y)\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regressionTI.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SXaNclTZSFjO"
      },
      "source": [
        "1. **Ответ:** Качество немного упало. Возможно используется много редких слов и почти всегда встречаются одинаковые слова, что делает их веса приблизительно одинаковыми и не несет ценности для линейной регрессии."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CPtk0lCA5POY"
      },
      "source": [
        "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU-MRXyRSHLm",
        "outputId": "6b5353a8-5f3b-4220-fcd4-64eecfe20054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество до: 51692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TechnoDX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество после: 51692\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=custom_tokenizer,\n",
        "    max_df=0.9\n",
        ")\n",
        "print(\"Количество до:\", train_X.count_nonzero())\n",
        "train_X, train_y, test_X, test_y = process(train_new, test_new, vectorizer)\n",
        "print(\"Количество после:\", train_X.count_nonzero())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z1TkO9HeSTJ9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7446\n"
          ]
        }
      ],
      "source": [
        "regressionTI = LogisticRegression(max_iter=1000, random_state=42)\n",
        "regressionTI.fit(train_X, train_y)\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regressionTI.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Two_O3rSVmh"
      },
      "source": [
        "2. **Ответ:** Значения df были порядка 0.01, так что добавление верхней границы не изменило число слов. Зато если поставить max_df = 0.01, то все будет хорошо и размер матрицы упадет от 51692 элементов до 40001 элемента, то есть где-то на 23%. F1 score при этом упадет до 70%, что не очень хорошо"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VhyjbI5X5QnG"
      },
      "source": [
        "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mNpIxv6SfKc",
        "outputId": "607ec22e-1fdb-4c29-bbc4-6447ed266349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество до: 51692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TechnoDX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество после: 37694\n",
            "F1 score: 0.7442\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=custom_tokenizer,\n",
        "    min_df=0.001\n",
        ")\n",
        "print(\"Количество до:\", train_X.count_nonzero())\n",
        "train_X, train_y, test_X, test_y = process(train_new, test_new, vectorizer)\n",
        "print(\"Количество после:\", train_X.count_nonzero())\n",
        "\n",
        "regressionTI = LogisticRegression(max_iter=1000, random_state=42)\n",
        "regressionTI.fit(train_X, train_y)\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regressionTI.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "imeD8skxSqdg"
      },
      "source": [
        "1. **Ответ:** Этот результат намного лучше, так как мы сохранили качество по F1 мере, при этом сократив размер массива на 37%! (размер я считаю с помощью `count_nonzero()`, чтобы получить количество ненулевых элементов в разряженной матрице)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1l1sx4nB3HvN"
      },
      "source": [
        "## Задание 10 (1 балл)\n",
        "\n",
        "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
        "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
        "\n",
        "1. Повторите задание 8 с HashingVectorizer, укажите количество фичей равное 5000.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qSoW894RXxz",
        "outputId": "720818f4-f774-4273-bb10-918ecc72ef44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TechnoDX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Новое количество: 51634\n",
            "F1 score: 0.726\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vectorizer = HashingVectorizer(\n",
        "    tokenizer=custom_tokenizer,\n",
        "    n_features=5000\n",
        ")\n",
        "train_X, train_y, test_X, test_y = process(train_new, test_new, vectorizer)\n",
        "print(\"Новое количество:\", train_X.count_nonzero())\n",
        "\n",
        "regressionTI = LogisticRegression(max_iter=1000, random_state=42)\n",
        "regressionTI.fit(train_X, train_y)\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regressionTI.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1C3I4ceg6AG-"
      },
      "source": [
        "2. Какой из подходов показал самый высокий результат?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_bIfyVlOS9Lu"
      },
      "source": [
        "1. **Ответ:** По прежнему самый весокий результат показывает исходная функция `CountVectorizer`. Хотя лучшей попыткой я бы назвал уменьшение размеров через задание min_df, при которой качество практически не изменилось, но размер матрицы уменьшился на 37%"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zylJ6l0R3HvN"
      },
      "source": [
        "## Задание 11 (1 балл)\n",
        "\n",
        "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GSTVApFeS-OY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TechnoDX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7515\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=custom_tokenizer)\n",
        "\n",
        "train_X = vectorizer.fit_transform(train.text) \n",
        "test_X = vectorizer.transform(test.text) \n",
        "train_y = train_new.target\n",
        "test_y = test_new.target\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "train_X = scaler.fit_transform(train_X)\n",
        "test_X = scaler.transform(test_X) \n",
        "\n",
        "regressionTI = LogisticRegression(max_iter=200, random_state=42)\n",
        "regressionTI.fit(train_X, train_y)\n",
        "\n",
        "print(\"F1 score:\", np.round(f1_score(test_y, regressionTI.predict(test_X)), 4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Все отлично. Я взял train без `'location'` и `'keyword'` и все прошло отлично. Видимо эти данные путали регрессию, заставляя переобучаться "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YlLemInT3HvL",
        "A8CPBUal3HvL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
